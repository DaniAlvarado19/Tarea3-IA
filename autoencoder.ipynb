{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tarea 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 26.4M/26.4M [00:06<00:00, 4.33MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 186kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 4.42M/4.42M [00:01<00:00, 2.59MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import hydra\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Verificar si se está ejecutando dentro de un notebook\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        return 'ipykernel' in sys.modules\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=8):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        elf.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # Capa adicional\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 4 * 4, 128),  # Ajustar tamaño si añades más capas\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decodificador: Añadir una capa convolucional adicional\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64 * 4 * 4),  # Ajustar tamaño si añades más capas\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, 4, 4)),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),  # Capa adicional\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)\n",
    "\n",
    "class LitAutoencoder(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super(LitAutoencoder, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = config.model.learning_rate\n",
    "        self.weight_decay = config.model.weight_decay\n",
    "        self.autoencoder = Autoencoder(latent_dim=config.model.latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.autoencoder(x)\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx, step_type):\n",
    "        x, _ = batch\n",
    "        z = self.autoencoder.encode(x)\n",
    "        x_hat = self.autoencoder.decode(z)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        \n",
    "        psnr = 20 * torch.log10(1.0 / torch.sqrt(loss))\n",
    "        latent_norm = torch.norm(z, dim=1).mean()\n",
    "        \n",
    "        self.log(f'{step_type}/loss', loss)\n",
    "        self.log(f'{step_type}/psnr', psnr)\n",
    "        self.log(f'{step_type}/latent_norm', latent_norm)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'psnr': psnr,\n",
    "            'reconstructions': x_hat,\n",
    "            'originals': x,\n",
    "            'latent': z\n",
    "        }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        results = self._common_step(batch, batch_idx, 'train')\n",
    "        return results['loss']\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, 'val')\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, 'test')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val/loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "class ReconstructionCallback(pl.Callback):\n",
    "    def __init__(self, val_samples, save_dir, val_loader, epoch_interval=5, num_samples=10):\n",
    "        super().__init__()\n",
    "        self.val_samples = val_samples\n",
    "        self.epoch_interval = epoch_interval\n",
    "        self.num_samples = num_samples\n",
    "        self.save_dir = save_dir\n",
    "        self.val_loader = val_loader\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch\n",
    "        if epoch % self.epoch_interval == 0 or epoch == trainer.max_epochs - 1:\n",
    "            val_samples = self.val_samples.to(pl_module.device)\n",
    "            reconstructed = pl_module(val_samples)\n",
    "            fig = self.plot_reconstruction(val_samples, reconstructed, epoch)\n",
    "            \n",
    "            save_path = os.path.join(self.save_dir, f'reconstruction_epoch_{epoch}.png')\n",
    "            plt.savefig(save_path)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            if epoch == trainer.max_epochs - 1:\n",
    "                self.visualize_latent_space(pl_module)\n",
    "    \n",
    "    def plot_reconstruction(self, originals, reconstructed, epoch):\n",
    "        fig = plt.figure(figsize=(20, 4))\n",
    "        plt.suptitle(f'Epoch {epoch}')\n",
    "        \n",
    "        originals = originals[:self.num_samples].cpu().detach()\n",
    "        reconstructed = reconstructed[:self.num_samples].cpu().detach()\n",
    "        \n",
    "        for i in range(self.num_samples):\n",
    "            ax = plt.subplot(2, self.num_samples, i + 1)\n",
    "            plt.imshow(originals[i].squeeze(0), cmap='gray')\n",
    "            if i == 0:\n",
    "                plt.title(\"Original\")\n",
    "            plt.axis(\"off\")\n",
    "            \n",
    "            ax = plt.subplot(2, self.num_samples, i + 1 + self.num_samples)\n",
    "            plt.imshow(reconstructed[i].squeeze(0), cmap='gray')\n",
    "            if i == 0:\n",
    "                plt.title(\"Reconstrucción\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def visualize_latent_space(self, pl_module):\n",
    "        latent_vectors = []\n",
    "        labels = []\n",
    "        \n",
    "        pl_module.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                x, y = batch\n",
    "                z = pl_module.autoencoder.encode(x.to(pl_module.device))\n",
    "                latent_vectors.append(z.cpu())\n",
    "                labels.extend(y.numpy())\n",
    "        \n",
    "        latent_vectors = torch.cat(latent_vectors, dim=0).numpy()\n",
    "        \n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        latent_2d = tsne.fit_transform(latent_vectors)\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap='tab10')\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title('Visualización t-SNE del Espacio Latente')\n",
    "        plt.savefig(os.path.join(self.save_dir, 'latent_space_tsne.png'))\n",
    "        plt.close()\n",
    "\n",
    "# Adaptar para Jupyter o script regular\n",
    "def main(cfg: DictConfig):\n",
    "    pl.seed_everything(cfg.seed)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cfg.dataset.normalize.mean, \n",
    "                           std=cfg.dataset.normalize.std)\n",
    "    ])\n",
    "    \n",
    "    full_train_dataset = datasets.FashionMNIST(\n",
    "        root=cfg.dataset.root,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    train_size = int((1 - cfg.dataset.val_split) * len(full_train_dataset))\n",
    "    val_size = len(full_train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset,\n",
    "        [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.dataset.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.dataset.num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg.dataset.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.dataset.num_workers\n",
    "    )\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(\n",
    "            root=cfg.dataset.root,\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transform\n",
    "        ),\n",
    "        batch_size=cfg.dataset.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.dataset.num_workers\n",
    "    )\n",
    "    \n",
    "    model = LitAutoencoder(cfg)\n",
    "    \n",
    "    val_samples, _ = next(iter(val_loader))\n",
    "    val_samples = val_samples[:cfg.visualization.num_samples]\n",
    "    \n",
    "    callbacks = [\n",
    "        ReconstructionCallback(\n",
    "            val_samples,\n",
    "            save_dir=cfg.visualization.save_dir,\n",
    "            val_loader=val_loader,\n",
    "            epoch_interval=cfg.visualization.epoch_interval,\n",
    "            num_samples=cfg.visualization.num_samples\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            monitor='val/loss',\n",
    "            dirpath=os.path.join(cfg.visualization.save_dir, 'checkpoints'),\n",
    "            filename='autoencoder-{epoch:02d}-{val_loss:.2f}',\n",
    "            save_top_k=3,\n",
    "            mode='min'\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val/loss',\n",
    "            patience=10,\n",
    "            mode='min'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=cfg.trainer.max_epochs,\n",
    "        callbacks=callbacks,\n",
    "        accelerator=cfg.trainer.accelerator,\n",
    "        devices=cfg.trainer.devices,\n",
    "        log_every_n_steps=cfg.trainer.log_every_n_steps,\n",
    "        gradient_clip_val=cfg.trainer.gradient_clip_val,\n",
    "        precision=cfg.trainer.precision,\n",
    "        check_val_every_n_epoch=cfg.trainer.check_val_every_n_epoch\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.test(model, test_loader)\n",
    "    \n",
    "    return model, trainer\n",
    "\n",
    "if is_notebook():\n",
    "    # Si es un notebook, se configura OmegaConf manualmente\n",
    "    cfg = OmegaConf.create({\n",
    "        'seed': 42,\n",
    "        'dataset': {\n",
    "            'name': 'FashionMNIST',\n",
    "            'root': './data',\n",
    "            'batch_size': 128,\n",
    "            'num_workers': 4,\n",
    "            'val_split': 0.2,\n",
    "            'normalize': {\n",
    "                'mean': [0.5],\n",
    "                'std': [0.5]\n",
    "            }\n",
    "        },\n",
    "        'model': {\n",
    "            'latent_dim': 8,\n",
    "            'learning_rate': 0.001,\n",
    "            'weight_decay': 1e-5\n",
    "        },\n",
    "        'trainer': {\n",
    "            'max_epochs': 50,\n",
    "            'accelerator': 'auto',\n",
    "            'devices': 1,\n",
    "            'log_every_n_steps': 50,\n",
    "            'gradient_clip_val': 0.5,\n",
    "            'precision': 32,\n",
    "            'check_val_every_n_epoch': 1\n",
    "        },\n",
    "        'visualization': {\n",
    "            'num_samples': 10,\n",
    "            'epoch_interval': 5,\n",
    "            'save_dir': 'visualization_results'\n",
    "        }\n",
    "    })\n",
    "    main(cfg)\n",
    "else:\n",
    "    # Si es un script regular, se usa Hydra\n",
    "    @hydra.main(config_path=\"conf\", config_name=\"config\", version_base=None)\n",
    "    def hydra_main(cfg: DictConfig):\n",
    "        main(cfg)\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        hydra_main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
