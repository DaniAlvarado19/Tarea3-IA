{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tarea 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:05<00:00, 4.50MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 195kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.42M/4.42M [00:01<00:00, 2.39MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 5.14MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type        | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | autoencoder | Autoencoder | 13.7 M | train\n",
      "----------------------------------------------------\n",
      "13.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.7 M    Total params\n",
      "54.804    Total estimated model params size (MB)\n",
      "37        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0723e241c44027951c7ad1e6f6b580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0497300358b457baf3d5825ae4ec4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160c0adab6b0407e92e5eef42b61f86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Early stopping conditioned on metric `val/loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `train/mse_loss`, `train/perceptual_loss`, `train/total_loss`, `train/psnr`, `train/latent_norm`, `val/mse_loss`, `val/perceptual_loss`, `val/total_loss`, `val/psnr`, `val/latent_norm`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 383\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_notebook():\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;66;03m# Si es un notebook, se configura OmegaConf manualmente\u001b[39;00m\n\u001b[0;32m    350\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m OmegaConf\u001b[38;5;241m.\u001b[39mcreate({\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m         }\n\u001b[0;32m    382\u001b[0m     })\n\u001b[1;32m--> 383\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# Si es un script regular, se usa Hydra\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;129m@hydra\u001b[39m\u001b[38;5;241m.\u001b[39mmain(config_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m\"\u001b[39m, config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, version_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhydra_main\u001b[39m(cfg: DictConfig):\n",
      "Cell \u001b[1;32mIn[10], line 343\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m    310\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    311\u001b[0m     ReconstructionCallback(\n\u001b[0;32m    312\u001b[0m         val_samples,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m     )\n\u001b[0;32m    330\u001b[0m ]\n\u001b[0;32m    332\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m    333\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_epochs,\n\u001b[0;32m    334\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m     check_val_every_n_epoch\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcheck_val_every_n_epoch\n\u001b[0;32m    341\u001b[0m )\n\u001b[1;32m--> 343\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model, test_loader)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, trainer\n",
      "File \u001b[1;32mc:\\Users\\dnlal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dnlal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\dnlal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dnlal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dnlal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1025\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dnlal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:206\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dnlal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:378\u001b[0m, in \u001b[0;36m_FitLoop.on_advance_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    376\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, monitoring_callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    377\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 378\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_epoch_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitoring_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_epoch_end()\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39m_num_ready_batches_reached():\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# if we are restarting and the above condition holds, it's because we are reloading an epoch-end checkpoint.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# since metric-based schedulers require access to metrics and those are not currently saved in the\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# checkpoint, the plateau schedulers shouldn't be updated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dnlal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:218\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[1;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 218\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\dnlal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\callbacks\\early_stopping.py:190\u001b[0m, in \u001b[0;36mEarlyStopping.on_train_epoch_end\u001b[1;34m(self, trainer, pl_module)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_on_train_epoch_end \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_skip_check(trainer):\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_early_stopping_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dnlal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\callbacks\\early_stopping.py:202\u001b[0m, in \u001b[0;36mEarlyStopping._run_early_stopping_check\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks whether the early stopping condition is met and if so tells the trainer to stop the training.\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m logs \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcallback_metrics\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mfast_dev_run \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_condition_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# disable early_stopping with fast_dev_run\u001b[39;49;00m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# short circuit if metric not present\u001b[39;00m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    207\u001b[0m current \u001b[38;5;241m=\u001b[39m logs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor]\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[1;32mc:\\Users\\dnlal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\callbacks\\early_stopping.py:153\u001b[0m, in \u001b[0;36mEarlyStopping._validate_condition_metric\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m monitor_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrict:\n\u001b[1;32m--> 153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    155\u001b[0m         rank_zero_warn(error_msg, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mRuntimeWarning\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Early stopping conditioned on metric `val/loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `train/mse_loss`, `train/perceptual_loss`, `train/total_loss`, `train/psnr`, `train/latent_norm`, `val/mse_loss`, `val/perceptual_loss`, `val/total_loss`, `val/psnr`, `val/latent_norm`"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import hydra\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Verificar si se está ejecutando dentro de un notebook\n",
    "def is_notebook() -> bool:\n",
    "    \"\"\"\n",
    "    Comprueba si el código se está ejecutando dentro de un Jupyter Notebook.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si es un notebook, False en caso contrario.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return 'ipykernel' in sys.modules\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=64):\n",
    "        \"\"\"\n",
    "        Inicializa el autoencoder con un espacio latente de dimensión especificada.\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int): Dimensión del espacio latente.\n",
    "        \"\"\"\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Definición del encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=1, padding=1),   # Salida: 32 x 28 x 28\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # Salida: 64 x 14 x 14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), # Salida: 128 x 7 x 7\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.latent_dim)\n",
    "        )\n",
    "\n",
    "        # Definición del decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128 * 7 * 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (128, 7, 7)),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1), # Salida: 64 x 14 x 14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),  # Salida: 32 x 28 x 28\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, padding=1),  # Salida: 1 x 28 x 28\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Codifica las imágenes de entrada en un espacio latente.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Imágenes de entrada.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Representaciones en el espacio latente.\n",
    "        \"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decodifica las representaciones latentes para reconstruir las imágenes.\n",
    "\n",
    "        Args:\n",
    "            z (Tensor): Representaciones en el espacio latente.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Imágenes reconstruidas.\n",
    "        \"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pasa adelante por el modelo completo (encoder y decoder).\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Imágenes de entrada.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Imágenes reconstruidas.\n",
    "        \"\"\"\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)\n",
    "\n",
    "class LitAutoencoder(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Módulo Lightning que envuelve el autoencoder y maneja el entrenamiento.\n",
    "\n",
    "        Args:\n",
    "            config (DictConfig): Configuración del modelo y entrenamiento.\n",
    "        \"\"\"\n",
    "        super(LitAutoencoder, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = config.model.learning_rate\n",
    "        self.weight_decay = config.model.weight_decay\n",
    "        self.autoencoder = Autoencoder(latent_dim=config.model.latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pasa adelante por el autoencoder.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Imágenes de entrada.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Imágenes reconstruidas.\n",
    "        \"\"\"\n",
    "        return self.autoencoder(x)\n",
    "\n",
    "    def _common_step(self, batch, batch_idx, step_type):\n",
    "        \"\"\"\n",
    "        Paso común para entrenamiento, validación y prueba.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): Batch de datos.\n",
    "            batch_idx (int): Índice del batch.\n",
    "            step_type (str): Tipo de paso ('train', 'val' o 'test').\n",
    "\n",
    "        Returns:\n",
    "            dict: Diccionario con pérdida y métricas.\n",
    "        \"\"\"\n",
    "        x, _ = batch\n",
    "        x = x.to(self.device)\n",
    "        z = self.autoencoder.encode(x)\n",
    "        x_hat = self.autoencoder.decode(z)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "\n",
    "        psnr = 20 * torch.log10(1.0 / torch.sqrt(loss))\n",
    "        latent_norm = torch.norm(z, dim=1).mean()\n",
    "\n",
    "        # Registro de métricas\n",
    "        self.log(f'{step_type}/loss', loss)\n",
    "        self.log(f'{step_type}/psnr', psnr)\n",
    "        self.log(f'{step_type}/latent_norm', latent_norm)\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'psnr': psnr,\n",
    "            'reconstructions': x_hat,\n",
    "            'originals': x,\n",
    "            'latent': z\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Paso de entrenamiento.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): Batch de datos.\n",
    "            batch_idx (int): Índice del batch.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Pérdida de entrenamiento.\n",
    "        \"\"\"\n",
    "        results = self._common_step(batch, batch_idx, 'train')\n",
    "        return results['loss']\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Paso de validación.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): Batch de datos.\n",
    "            batch_idx (int): Índice del batch.\n",
    "        \"\"\"\n",
    "        self._common_step(batch, batch_idx, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Paso de prueba.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): Batch de datos.\n",
    "            batch_idx (int): Índice del batch.\n",
    "        \"\"\"\n",
    "        self._common_step(batch, batch_idx, 'test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configura los optimizadores y programadores de tasa de aprendizaje.\n",
    "\n",
    "        Returns:\n",
    "            dict: Diccionario con optimizador y programador.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val/loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "class ReconstructionCallback(pl.Callback):\n",
    "    def __init__(self, val_samples, save_dir, val_loader, epoch_interval=5, num_samples=10):\n",
    "        \"\"\"\n",
    "        Callback para visualizar reconstrucciones y el espacio latente.\n",
    "\n",
    "        Args:\n",
    "            val_samples (Tensor): Muestras de validación.\n",
    "            save_dir (str): Directorio para guardar resultados.\n",
    "            val_loader (DataLoader): DataLoader de validación.\n",
    "            epoch_interval (int): Intervalo de épocas para visualizar.\n",
    "            num_samples (int): Número de muestras a visualizar.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.val_samples = val_samples\n",
    "        self.epoch_interval = epoch_interval\n",
    "        self.num_samples = num_samples\n",
    "        self.save_dir = save_dir\n",
    "        self.val_loader = val_loader\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        \"\"\"\n",
    "        Evento al final de cada época de validación.\n",
    "\n",
    "        Args:\n",
    "            trainer (Trainer): Instancia del entrenador.\n",
    "            pl_module (LightningModule): Modelo actual.\n",
    "        \"\"\"\n",
    "        epoch = trainer.current_epoch\n",
    "        if epoch % self.epoch_interval == 0 or epoch == trainer.max_epochs - 1:\n",
    "            val_samples = self.val_samples.to(pl_module.device)\n",
    "            reconstructed = pl_module(val_samples)\n",
    "            fig = self.plot_reconstruction(val_samples, reconstructed, epoch)\n",
    "\n",
    "            save_path = os.path.join(self.save_dir, f'reconstruction_epoch_{epoch}.png')\n",
    "            plt.savefig(save_path)\n",
    "            plt.close(fig)\n",
    "\n",
    "            if epoch == trainer.max_epochs - 1:\n",
    "                self.visualize_latent_space(pl_module)\n",
    "\n",
    "    def plot_reconstruction(self, originals, reconstructed, epoch):\n",
    "        \"\"\"\n",
    "        Genera y guarda una figura comparando originales y reconstrucciones.\n",
    "\n",
    "        Args:\n",
    "            originals (Tensor): Imágenes originales.\n",
    "            reconstructed (Tensor): Imágenes reconstruidas.\n",
    "            epoch (int): Época actual.\n",
    "\n",
    "        Returns:\n",
    "            Figure: Figura generada.\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(20, 4))\n",
    "        plt.suptitle(f'Epoch {epoch}')\n",
    "\n",
    "        originals = originals[:self.num_samples].cpu().detach()\n",
    "        reconstructed = reconstructed[:self.num_samples].cpu().detach()\n",
    "\n",
    "        for i in range(self.num_samples):\n",
    "            # Imágenes originales\n",
    "            ax = plt.subplot(2, self.num_samples, i + 1)\n",
    "            plt.imshow(originals[i].squeeze(0), cmap='gray')\n",
    "            if i == 0:\n",
    "                plt.title(\"Original\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Imágenes reconstruidas\n",
    "            ax = plt.subplot(2, self.num_samples, i + 1 + self.num_samples)\n",
    "            plt.imshow(reconstructed[i].squeeze(0), cmap='gray')\n",
    "            if i == 0:\n",
    "                plt.title(\"Reconstrucción\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def visualize_latent_space(self, pl_module):\n",
    "        \"\"\"\n",
    "        Visualiza el espacio latente usando t-SNE al final del entrenamiento.\n",
    "\n",
    "        Args:\n",
    "            pl_module (LightningModule): Modelo entrenado.\n",
    "        \"\"\"\n",
    "        latent_vectors = []\n",
    "        labels = []\n",
    "\n",
    "        pl_module.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                x, y = batch\n",
    "                z = pl_module.autoencoder.encode(x.to(pl_module.device))\n",
    "                latent_vectors.append(z.cpu())\n",
    "                labels.extend(y.numpy())\n",
    "\n",
    "        latent_vectors = torch.cat(latent_vectors, dim=0).numpy()\n",
    "\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        latent_2d = tsne.fit_transform(latent_vectors)\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap='tab10')\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title('Visualización t-SNE del Espacio Latente')\n",
    "        plt.savefig(os.path.join(self.save_dir, 'latent_space_tsne.png'))\n",
    "        plt.close()\n",
    "\n",
    "# Adaptar para Jupyter o script regular\n",
    "def main(cfg: DictConfig):\n",
    "    \"\"\"\n",
    "    Función principal que configura los datos, el modelo y ejecuta el entrenamiento.\n",
    "\n",
    "    Args:\n",
    "        cfg (DictConfig): Configuración completa del experimento.\n",
    "    \"\"\"\n",
    "    pl.seed_everything(cfg.seed)\n",
    "\n",
    "    # Transformaciones y normalización\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # No se realiza normalización adicional porque la salida es entre 0 y 1\n",
    "    ])\n",
    "\n",
    "    # Carga del conjunto de datos completo de entrenamiento\n",
    "    full_train_dataset = datasets.FashionMNIST(\n",
    "        root=cfg.dataset.root,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # División en entrenamiento y validación\n",
    "    train_size = int((1 - cfg.dataset.val_split) * len(full_train_dataset))\n",
    "    val_size = len(full_train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset,\n",
    "        [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    # DataLoader para entrenamiento\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.dataset.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.dataset.num_workers\n",
    "    )\n",
    "\n",
    "    # DataLoader para validación\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg.dataset.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.dataset.num_workers\n",
    "    )\n",
    "\n",
    "    # DataLoader para prueba\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(\n",
    "            root=cfg.dataset.root,\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transform\n",
    "        ),\n",
    "        batch_size=cfg.dataset.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.dataset.num_workers\n",
    "    )\n",
    "\n",
    "    # Inicialización del modelo\n",
    "    model = LitAutoencoder(cfg)\n",
    "\n",
    "    # Muestras para visualización de reconstrucciones\n",
    "    val_samples, _ = next(iter(val_loader))\n",
    "    val_samples = val_samples[:cfg.visualization.num_samples]\n",
    "\n",
    "    # Definición de callbacks\n",
    "    callbacks = [\n",
    "        ReconstructionCallback(\n",
    "            val_samples,\n",
    "            save_dir=cfg.visualization.save_dir,\n",
    "            val_loader=val_loader,\n",
    "            epoch_interval=cfg.visualization.epoch_interval,\n",
    "            num_samples=cfg.visualization.num_samples\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            monitor='val/loss',\n",
    "            dirpath=os.path.join(cfg.visualization.save_dir, 'checkpoints'),\n",
    "            filename='autoencoder-{epoch:02d}-{val_loss:.2f}',\n",
    "            save_top_k=3,\n",
    "            mode='min'\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val/loss',\n",
    "            patience=10,\n",
    "            mode='min'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Configuración del entrenador\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=cfg.trainer.max_epochs,\n",
    "        callbacks=callbacks,\n",
    "        accelerator=cfg.trainer.accelerator,\n",
    "        devices=cfg.trainer.devices,\n",
    "        log_every_n_steps=cfg.trainer.log_every_n_steps,\n",
    "        gradient_clip_val=cfg.trainer.gradient_clip_val,\n",
    "        precision=cfg.trainer.precision,\n",
    "        check_val_every_n_epoch=cfg.trainer.check_val_every_n_epoch\n",
    "    )\n",
    "\n",
    "    # Entrenamiento del modelo\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    # Prueba del modelo\n",
    "    trainer.test(model, test_loader)\n",
    "\n",
    "    return model, trainer\n",
    "\n",
    "if is_notebook():\n",
    "    # Si es un notebook, se configura OmegaConf manualmente\n",
    "    cfg = OmegaConf.create({\n",
    "        'seed': 42,\n",
    "        'dataset': {\n",
    "            'name': 'FashionMNIST',\n",
    "            'root': './data',\n",
    "            'batch_size': 128,\n",
    "            'num_workers': 4,\n",
    "            'val_split': 0.2,\n",
    "            'normalize': {\n",
    "                'mean': [0.5],\n",
    "                'std': [0.5]\n",
    "            }\n",
    "        },\n",
    "        'model': {\n",
    "            'latent_dim': 64,  # Aumentamos la dimensión latente\n",
    "            'learning_rate': 0.001,\n",
    "            'weight_decay': 1e-5\n",
    "        },\n",
    "        'trainer': {\n",
    "            'max_epochs': 100,  # Aumentamos el número de épocas\n",
    "            'accelerator': 'auto',\n",
    "            'devices': 1,\n",
    "            'log_every_n_steps': 50,\n",
    "            'gradient_clip_val': 0.5,\n",
    "            'precision': 32,\n",
    "            'check_val_every_n_epoch': 1\n",
    "        },\n",
    "        'visualization': {\n",
    "            'num_samples': 10,\n",
    "            'epoch_interval': 5,\n",
    "            'save_dir': 'visualization_results'\n",
    "        }\n",
    "    })\n",
    "    main(cfg)\n",
    "else:\n",
    "    # Si es un script regular, se usa Hydra\n",
    "    @hydra.main(config_path=\"conf\", config_name=\"config\", version_base=None)\n",
    "    def hydra_main(cfg: DictConfig):\n",
    "        \"\"\"\n",
    "        Función principal para ejecución con Hydra.\n",
    "\n",
    "        Args:\n",
    "            cfg (DictConfig): Configuración completa del experimento.\n",
    "        \"\"\"\n",
    "        main(cfg)\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        hydra_main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
